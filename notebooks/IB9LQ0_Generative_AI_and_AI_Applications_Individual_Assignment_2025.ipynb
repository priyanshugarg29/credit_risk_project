{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/github/priyanshugarg29/credit_risk_project/blob/main/IB9LQ0_Generative_AI_and_AI_Applications_Individual_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "wg80z6eQu_Jg"
   },
   "outputs": [],
   "source": [
    "# IB9LQ0 Generative AI and AI Applications Individual Assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "j2ShFFcOuVdX"
   },
   "outputs": [],
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "cvFsnfR4x2kI"
   },
   "outputs": [],
   "source": [
    "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating natural language responses across a wide range of domains. However, they often fall short when responding to queries that depend on specialized, proprietary, or up-to-date knowledge that lies outside their training data. This limitation can result in hallucinations, outdated reasoning, or misinterpretations — particularly in domains governed by complex regulations and institutional policies.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) offers a powerful solution by combining the fluency of LLMs with the factual grounding of document retrieval. In a RAG system, relevant passages are retrieved from an external corpus and provided as context to the LLM, enabling accurate, transparent, and verifiable responses.\n",
    "\n",
    "This project explores the application of RAG in the credit risk domain, focusing on lending decisions across personal loans, auto finance, and SME credit. These are areas where interpretability, consistency, and regulatory compliance are critical — and where domain-specific knowledge must be integrated into decision support tools.\n",
    "\n",
    "The goal of this project is to implement and evaluate a cross-sector RAG system that retrieves and grounds responses using internal credit policy documents inspired by UK regulatory standards (FCA CONC). This approach demonstrates the system’s potential to enhance decision quality in financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "lXblX8vQv6E0"
   },
   "outputs": [],
   "source": [
    "## 2. Domain Selection & Dataset Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "E8KeMtI9z2Pt"
   },
   "outputs": [],
   "source": [
    "This project is situated in the domain of credit risk and lending policy interpretation, where consistent, compliant, and explainable decision-making is critical. Financial institutions rely on complex internal policies to assess creditworthiness, affordability, and risk. However, these documents are often unstructured and inaccessible to automated systems. Standalone LLMs struggle to interpret such policies without hallucinating or misapplying regulatory logic. A Retrieval-Augmented Generation (RAG) system is well-suited here, enabling grounded and auditable responses to nuanced credit-related queries.\n",
    "\n",
    "To simulate a realistic application scenario, a custom dataset was synthesized using GPT-4, based on publicly available FCA guidance — specifically the Consumer Credit Sourcebook (CONC) (*Financial Conduct Authority (FCA),2024*), including section 5.2A on creditworthiness. The dataset was generated using a domain-specific prompt (*APPENDIX A*) instructing the model to translate formal regulations into internal bank-style credit policies, across multiple product lines.\n",
    "\n",
    "The final dataset consists of six structured .txt files, covering personal loans, auto finance, SME lending, approval matrices, exception handling, and data privacy. Each file is written in clear, operational language suitable for chunking and embedding. This design supports accurate retrieval and LLM reasoning within the RAG framework."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "W3WsGilg8mzv"
   },
   "outputs": [],
   "source": [
    "## 3. System Architecture\n",
    "\n",
    "This project implements a cross-sector Retrieval-Augmented Generation (RAG)\n",
    "system designed to interpret internal credit risk policies inspired by UK lending regulations. The system follows a modular pipeline architecture, aligning with best practices from lecture materials.\n",
    "\n",
    "The pipeline begins with loading synthetic domain documents from a GitHub repository. These documents simulate internal credit policies for personal loans, auto finance, and SME lending, generated using FCA’s CONC guidelines as a regulatory foundation.\n",
    "\n",
    "Next, the documents are processed through semantic chunking using LangChain’s RecursiveCharacterTextSplitter, which breaks each policy into overlapping segments of 500 characters with 100-character overlap. This ensures that meaningful context boundaries (e.g., eligibility clauses, document requirements) are preserved during retrieval.\n",
    "\n",
    "The chunks are then embedded using a local transformer model (BAAI/bge-base-en), as explored in lecture workshops. These embeddings are stored in a FAISS vector index, enabling fast and scalable similarity search.\n",
    "\n",
    "To generate answers, a retriever is initialized on the FAISS index. Given a user query, it retrieves the top-k relevant chunks. These chunks are manually formatted into a prompt and passed to the Gemini LLM (gemini-2.0-flash) via a REST API call, bypassing the need for OpenAI or Hugging Face-based hosting.\n",
    "\n",
    "The generated answer is then displayed alongside the original query. This setup enables the system to respond only with grounded, document-supported answers, minimizing hallucination and maximizing transparency — key requirements for credit-risk applications.\n",
    "\n",
    "The entire architecture is modular, interpretable, and extensible — capable of supporting future enhancements such as reranking, multi-hop retrieval, or streamlining via LangChain once Gemini integration is available."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "JAlCDLrM9wDc"
   },
   "outputs": [],
   "source": [
    "## 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "u0CznOSqwKiX"
   },
   "outputs": [],
   "source": [
    "### Step 1: Loading the documents from GitHub\n",
    "\n",
    "Retrieving the synthesized credit policy documents hosted on GitHub. These documents represent internal lending guidelines inspired by UK regulatory standards (primarily FCA CONC 5.2A) across multiple product types — personal loans, auto loans, and SME lending.\n",
    "\n",
    "They will be used as the foundation for chunking, embedding, and semantic retrieval in our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAN7U47LwoAV",
    "outputId": "147b8c7b-dac2-4f79-9791-779924df82bb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Base URL from your GitHub repo\n",
    "base_url = \"https://raw.githubusercontent.com/priyanshugarg29/credit_risk_project/main/data/\"\n",
    "\n",
    "# List of policy files synthesized and placed at mentioned repo\n",
    "files = [\n",
    "    \"personal_loans_policy.txt\",\n",
    "    \"auto_loans_policy.txt\",\n",
    "    \"sme_lending_policy.txt\",\n",
    "    \"approval_matrix.txt\",\n",
    "    \"risk_flags_and_exceptions.txt\",\n",
    "    \"privacy_and_data_use_policy.txt\"\n",
    "]\n",
    "\n",
    "# Loading the documents\n",
    "documents = []\n",
    "for file in files:\n",
    "    url = base_url + file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        documents.append(response.text)\n",
    "        print(f\"Successfully Loaded: {file}\")\n",
    "    else:\n",
    "        print(f\"Failed to load: {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "cCLlV7E01YnY"
   },
   "outputs": [],
   "source": [
    "### Step 2: Semantic Chunking of Policy Documents\n",
    "\n",
    "After loading the domain-specific credit policy documents, the next step is to split the text into **retrievable chunks** for embedding and vector search.\n",
    "\n",
    "This process, known as **semantic chunking**, ensures that:\n",
    "- Each chunk is small enough to fit within the LLM’s context window\n",
    "- Logical sections (e.g., rules, bullet points, paragraphs) are preserved\n",
    "- The system retrieves contextually relevant information instead of isolated fragments\n",
    "\n",
    "To achieve this, we use **LangChain’s `RecursiveCharacterTextSplitter`**, which intelligently breaks down text based on sentence structure and document layout. Each chunk is linked to its source document, allowing the system to later **cite or trace** where the information was retrieved from.\n",
    "\n",
    "The output of this step is a list of **overlapping, semantically meaningful chunks** (typically 300–500 characters), which will be embedded into a vector database for fast and accurate retrieval in the RAG pipeline.\n",
    "\n",
    "We chose **LangChain's `RecursiveCharacterTextSplitter`** over fixed-length or naive sentence-based splitting because it better preserves the semantic structure of regulatory documents. These policies often contain numbered sections, nested rules, and formal phrasing that would be lost or fragmented with basic character-based splitting.\n",
    "\n",
    "The recursive splitter prioritizes logical boundaries (e.g., paragraphs, bullet points, newline spacing) and falls back to character length only when necessary. This makes it well-suited to our domain, where **contextual integrity** is critical — for example, when retrieving eligibility criteria, escalation thresholds, or exception conditions.\n",
    "\n",
    "By using overlapping chunks (with a small buffer), we also reduce the likelihood that important cross-line references (e.g., “see 1.3 above”) are missed during retrieval. This enhances the quality of downstream LLM responses, especially when tracing policy logic in credit risk scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oDTbuZp2ES7",
    "outputId": "ee7ec624-0626-48f4-96ad-2e2b62dfd8be"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "#Creating LangChain Document objects with metadata\n",
    "docs = []\n",
    "for file_name, content in zip(files, documents):\n",
    "    doc = Document(\n",
    "        page_content=content,\n",
    "        metadata={\"source\": file_name}\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "#Initializing the chunker\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # target chunk size\n",
    "    chunk_overlap=100     # overlap between chunks to retain context\n",
    ")\n",
    "\n",
    "#Spliting each document into chunks\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 4: Preview\n",
    "print(f\"Total chunks created: {len(chunked_docs)}\")\n",
    "print(\"Sample chunk:\")\n",
    "print(\"Source:\", chunked_docs[0].metadata['source'])\n",
    "print(chunked_docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "K99APV8s3dSa"
   },
   "outputs": [],
   "source": [
    "### Step 3: Embedding Policy Chunks and Building the FAISS Vector Store\n",
    "\n",
    "**GOAL:** Converting the semantic chunks into vector embeddings using a transformer model and storing them in a vector database (like FAISS or Chroma) for fast semantic search during retrieval.\n",
    "\n",
    "\n",
    "**HOW THIS IS ACHIEVED:** To enable efficient semantic search, we convert each document chunk into a numerical vector using a locally hosted sentence transformer model. We use **BAAI/bge-base-en**, which has been shown in lecture experiments to perform well on short formal text typical of policy and legal content.\n",
    "\n",
    "The embedded vectors are then stored in a **FAISS vector database**, which supports fast approximate nearest-neighbor search. This setup allows the RAG system to quickly retrieve the most relevant chunks at inference time without relying on external APIs.\n",
    "\n",
    "Using a local model ensures the system is fully deployable without cloud dependency and avoids token usage costs or data privacy concerns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnnVpkFU5Ufq",
    "outputId": "80aa045a-8973-42f4-f899-a9da3c38cd84"
   },
   "outputs": [],
   "source": [
    "#Installing necessary libraries\n",
    "!pip install langchain\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t39W8YFU4_Sj",
    "outputId": "224f65db-7766-439f-af06-17f06687100d"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#Initializing the embedding model (same as used in lecture notebooks)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en\",\n",
    "    model_kwargs={\"device\": \"cpu\"},   # or \"cuda\" if running on GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Creating FAISS vector store\n",
    "vectorstore = FAISS.from_documents(chunked_docs, embedding_model)\n",
    "\n",
    "# Saving the FAISS index (for persistence)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# Previewing implementation\n",
    "print(\"Embedding completed and FAISS index created.\")\n",
    "print(f\"Total vectors stored: {vectorstore.index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "vexgpr8DucEG"
   },
   "outputs": [],
   "source": [
    "### Step 4: Retrieval and Generation using Gemini API and FAISS\n",
    "\n",
    "In this step, we integrate our FAISS-based retrieval pipeline with Google's Gemini (`gemini-2.0-flash`) model to enable grounded text generation.\n",
    "\n",
    "The process is as follows:\n",
    "1. Retrieve the most relevant policy chunks using semantic similarity search.\n",
    "2. Combine the retrieved content with the user’s query to form a prompt.\n",
    "3. Send the prompt to Gemini’s API for generation.\n",
    "\n",
    "This design keeps retrieval logic separate from generation, allowing flexible integration of any LLM backend. Using Gemini instead of GPT demonstrates the modular nature of the RAG architecture and supports use cases where Google’s ecosystem is preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMzzDuxPu2ag",
    "outputId": "ed6bf00c-f91a-4d23-a5ac-b34cf366bdd8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Gemini API Key\n",
    "GEMINI_API_KEY = \n",
    "\n",
    "# Defining a retrieval + generation function using Gemini\n",
    "def query_with_gemini(user_query, retriever):\n",
    "    # Step 1: Retrieving top-k relevant chunks\n",
    "    retrieved_docs = retriever.get_relevant_documents(user_query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Step 2: Formatting RAG prompt manually\n",
    "    prompt = f\"\"\"\n",
    "You are a credit policy assistant. Use the provided policy documents to answer the user's question.\n",
    "If the answer is not available in the documents, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Step 3: Sending the prompt to Gemini API\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}\"\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Step 4: Returning or logging error while fetching result\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['candidates'][0]['content']['parts'][0]['text']\n",
    "    else:\n",
    "        print(\"Gemini API call failed:\", response.text)\n",
    "        return None\n",
    "\n",
    "# Testing a query\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "print(\"Checking for a query relevant to the documents\")\n",
    "query = \"What documents are required for SME lending\"\n",
    "response = query_with_gemini(query, retriever)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Gemini Response:\\n\", response)\n",
    "\n",
    "\n",
    "print(\"Checking for a query irrelevant to the documents\")\n",
    "query = \"What is the date today\"\n",
    "response = query_with_gemini(query, retriever)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Gemini Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "yyeeV-wN0Q8j"
   },
   "outputs": [],
   "source": [
    "### Step 5: Evaluation – Multi-query Testing and Response Logging\n",
    "\n",
    "To evaluate the performance of the RAG system, we define a set of realistic test queries that reflect common questions a credit risk analyst or loan officer might ask.\n",
    "\n",
    "These queries span multiple product domains (personal loans, auto finance, SME lending) and policy layers (eligibility, exceptions, risk flags, approvals, privacy).\n",
    "\n",
    "Each query is processed through the system, and its response is logged for qualitative evaluation. The aim is to assess:\n",
    "- Relevance of the answer to the query\n",
    "- Grounding in policy context\n",
    "- Completeness and clarity\n",
    "- Hallucination resistance\n",
    "\n",
    "This setup enables structured comparison and supports rubric-based scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lsm690_B0WIU",
    "outputId": "012051a4-42e9-419e-ad7b-190284f8b298"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Defining realistic credit policy queries\n",
    "test_queries = [\n",
    "    \"What is the minimum income required for a personal loan?\",\n",
    "    \"Can a borrower with a credit score of 580 be approved for any product?\",\n",
    "    \"What documents are needed for auto loan processing?\",\n",
    "    \"Who can approve loans above £25,000?\",\n",
    "    \"What happens if the debt-to-income ratio exceeds 50%?\",\n",
    "    \"Are startups eligible for SME loans?\",\n",
    "    \"How should Open Banking data be handled under your policies?\",\n",
    "    \"What are the risk flags for personal loan applicants?\",\n",
    "    \"What is required from self-employed applicants applying for a personal loan?\",\n",
    "    \"Can a customer with a high-performance car apply for auto finance?\"\n",
    "]\n",
    "\n",
    "# Running all queries and log responses\n",
    "results = []\n",
    "for i, q in enumerate(test_queries, start=1):\n",
    "    print(f\"\\nQuery {i}: {q}\")\n",
    "    res = query_with_gemini(q, retriever)\n",
    "    print(\"Gemini Response:\\n\", res)\n",
    "    results.append({\n",
    "        \"Query\": q,\n",
    "        \"Response\": res\n",
    "    })\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Kgt726576wvW"
   },
   "outputs": [],
   "source": [
    "#### Manual Scoring Rubric\n",
    "\n",
    "Each response is scored on a 0–2 scale across four dimensions:\n",
    "\n",
    "1. **Relevance** - The answer aligns with the query\n",
    "2. **Grounding** - The response is supported by retrieved context\n",
    "3. **Completeness** - All critical elements are addressed\n",
    "4. **Clarity** - Response is logically structured and readable\n",
    "\n",
    "This qualitative evaluation helps measure whether the RAG system meets its goal of generating grounded, domain-specific answers.\n",
    "\n",
    "| Criterion        | Score Range | Description                                                  |\n",
    "| ---------------- | ----------- | ------------------------------------------------------------ |\n",
    "| **Relevance**    | 0-2         | Is the response on-topic and directly answers the query?     |\n",
    "| **Grounding**    | 0-2         | Is the answer clearly supported by the retrieved documents?  |\n",
    "| **Completeness** | 0-2         | Does it cover all key points without being vague or partial? |\n",
    "| **Clarity**      | 0-2         | Is the answer well-structured and easy to understand?        |\n",
    "\n",
    "Max per query = 8 points\n",
    "\n",
    "| #  | Query                                                                  | Relevance | Grounding | Completeness | Clarity | Total |\n",
    "| -- | ---------------------------------------------------------------------- | --------- | --------- | ------------ | ------- | ----- |\n",
    "| 1  | What is the minimum income required for a personal loan?               | 2         | 2         | 2            | 2       | **8** |\n",
    "| 2  | Can a borrower with a credit score of 580 be approved for any product? | 2         | 2         | 2            | 2       | **8** |\n",
    "| 3  | What documents are needed for auto loan processing?                    | 2         | 2         | 2            | 2       | **8** |\n",
    "| 4  | Who can approve loans above £25,000?                                   | 1         | 1         | 1            | 2       | **5** |\n",
    "| 5  | What happens if the debt-to-income ratio exceeds 50%?                  | 2         | 1         | 1            | 2       | **6** |\n",
    "| 6  | Are startups eligible for SME loans?                                   | 2         | 2         | 2            | 2       | **8** |\n",
    "| 7  | How should Open Banking data be handled under your policies?           | 2         | 2         | 2            | 2       | **8** |\n",
    "| 8  | What are the risk flags for personal loan applicants?                  | 2         | 2         | 2            | 2       | **8** |\n",
    "| 9  | What is required from self-employed applicants for a personal loan?    | 0         | 0         | 0            | 1       | **1** |\n",
    "| 10 | Can a customer with a high-performance car apply for auto finance?     | 2         | 2         | 2            | 2       | **8** |\n",
    "\n",
    "Average Score: 68 / 80 = 85%\n",
    "\n",
    "Strongest performance: Queries 1, 2, 3, 6, 7, 8, 10 (all perfect)\n",
    "\n",
    "Weaker performance: Query 4 (ambiguous context) and Query 9 (retrieval failure)\n",
    "\n",
    "Interpretation\n",
    "\n",
    "This evaluation demonstrates that the RAG system:\n",
    "\n",
    "- Responds consistently and accurately to domain-specific questions\n",
    "\n",
    "- Grounds responses effectively in retrieved content\n",
    "\n",
    "- Handles irrelevant or ambiguous queries without hallucination\n",
    "\n",
    "- Has room for improvement in edge-case recall (e.g., retrieval miss in Q9)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "LyZjW1RsHZJO"
   },
   "outputs": [],
   "source": [
    "### STEP 6: Optional Evaluation: Measuring Semantic Similarity with BERTScore\n",
    "\n",
    "In addition to rubric-based evaluation, we use **BERTScore** to quantify the semantic similarity between generated responses and ideal reference answers.\n",
    "\n",
    "BERTScore compares token embeddings from a transformer model and evaluates how well the generated answer aligns **semantically** with a reference, rather than just matching words. This is particularly useful in open-ended response generation.\n",
    "\n",
    "We compare:\n",
    "- LLM-only response (without document context)\n",
    "- RAG response (using retrieved policy chunks)\n",
    "- Reference answer (written based on the source documents)\n",
    "\n",
    "This helps assess how much **semantic accuracy improves** when document grounding is introduced through RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiI0HE7PIIPH",
    "outputId": "f55b6d57-5161-45e6-e8eb-bcda4570bad8"
   },
   "outputs": [],
   "source": [
    "!pip install -q bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652,
     "referenced_widgets": [
      "00a6b0dc8a5545d1b7a616073bb8d940",
      "d727a883915f43568336df974c23cca0",
      "d2f9e39431f24fccacb718183999321e",
      "5f27fc0b8df54e47b7a0cf60fc27066c",
      "b3c8afd5d7834da39603c37a1a2030d7",
      "cada5bd63ef346d4931b0389fe5699bc",
      "d70cbf5cc416437b9b28d28133d93196",
      "ce8a29cb686940978eec8ffad9180b9a",
      "c28a4d0fda99408f8157876c493a8868",
      "5ef34bbef98f4558b0890ff36b330a30",
      "5e5017093aed4184a027a9058f470193",
      "54230eb63b7348e487a6929bddae9a4d",
      "e2d00762497445d0a7e15dc204bedc9a",
      "714343e24fa34bf6bd77db1a0c117dd7",
      "7ba072500dd8453ea62242ba576a306d",
      "375edf2b69ff4bc392ba5edb53723e35",
      "f2bb872f06ce47f190a06460c9d67395",
      "9c8da31824234dc1942110bed412da86",
      "16c6973415514789b8cf312a1d2cf92a",
      "1cdd5cafbbb44535a13ca1ee9efde54f",
      "72f29d182d604e8f9a88216ee5a2ca88",
      "a0f5859622914f38884e982c7947f096",
      "ce4efff4b10c498197c4797daba824c1",
      "fac7070c95034e6987fa2a94064b74a4",
      "38d87fa164604332bf38f89a22841adb",
      "866ca805702b48dd8d31235a3150a7f5",
      "08a2ee4b701f4c11b124022275c51812",
      "eb69c0c3de734b6583b3c65a58ba9cb3",
      "6dcecb1a983249f3b4930f2612c43f02",
      "8dfaeb84fd864ae68b0135fda1d1ccfd",
      "f8c778ea418f4dc99639d45ea4d94b29",
      "2e907ef625a6442e88e2768527039542",
      "42d06e9afe8348c98a7fe927af9cce3a",
      "2f3dc2eb534d4509952f4e470a56d394",
      "eec2f16ef24d45d7978dd9b240d1fe6d",
      "8bfe6efe9e41456da17a4e645246fe18",
      "0e4ead0a44bc4502a985264e343bc90b",
      "81c3a74ad47b4cb8827c593d1f4f1fa2",
      "d6fbce08d9a645dc84b78c30858b749c",
      "f6a3980d311841088168fdbdc457cf8f",
      "f52cd24151624dbda00e7d275189652f",
      "9d87c65d1a21468e8cb78e3f8ee373c6",
      "a62e1fb317804432bbde8a7164979bc4",
      "b63e073e994747019f610e7c6fe2c5a7",
      "c331901f4ab8450e9c705a6fa3f9e507",
      "7c1b5dc794ec4e8399297d7192c9d43a",
      "aa96b02ba7c242b7b5d4506ee07ad03f",
      "e4adee2845cf457982ad76df4a8af8f1",
      "c4461725ea93432395796119b6220906",
      "63c8b178418945efa42470fa8bb3daba",
      "3ea0f14cdf19434a815c3cc21f7a2be8",
      "9bc733c99313497bb6bf062492c309e8",
      "78a7574cdaff467cb277309f3d952bce",
      "5e48a503887541ac8da8fb7d6866ed9d",
      "b34fde633b1a458d85f5207759250c42",
      "1a6ac4cd2d4a43a399da3ccad736033a",
      "568205dfb56f415bba41f742f09bdc01",
      "34a5c54234d043a4809db550a3d25732",
      "b5682a80a67c4592a132ba5e855b9676",
      "0c4a6515574a46a2a471664c6ab07fe2",
      "4114ce5e3bec47efb005723b04786bd4",
      "3e44eabbb94d49189af19c067428d0eb",
      "cf1bfb318af6456c996d19876fdb9371",
      "1b0c6f10b1f74dcfafedf3cb5bc2efa7",
      "03548256eeaa4cf59ded80391bdaa111",
      "812b8b1099ee4574b871c57c57f2226d",
      "c3d71ba85da74f388cbbb59951328c21",
      "393d315a9e19479197419554ea971e8c",
      "c128b60304c4422dafbd0f5b6993b332",
      "eb14fddc4b6140e68fef2e3304dcc3cd",
      "690905331cac49a6b61792f7a6a1e3ad",
      "8f1e2b3bed6b4301b41a545dd6ef106c",
      "1770ab5f16694721be3dbfb937e14fe3",
      "f66e338e7a574bdb8730bf9ffdec3010",
      "332417601bc0407f874f37a567921347",
      "b90f4da858cd49be8beede1db690bf7e",
      "3fb5475e7d424db4a66cef996423aa9b",
      "7f46c21fe9214b3c994c25c8b0c4d63e",
      "12e12a931bfb413383ae50e1ce5ab322",
      "ed2202e9fb9a4f8aa2e536bd8e9d3a19",
      "23f709dcff434b3e99ce6a79b3a86b97",
      "d20e55014b1247e8bc536eaf2d12ed65",
      "341b9d7063b141b1939e23f44caa6394",
      "37fa46ede41b4f999bfa74178a9abc96",
      "d09e0d557a0e4b1d895b416879dbb040",
      "4a86d9d53f6e42f5b78449da23075fcc",
      "a83c68efd55c41d085ce43865f9a2a6e",
      "38aa2b4ebbdb4d74b9268b5a7fcad82c",
      "7f3ed8eaf81f4e13813998cb1b9f593c",
      "ade52c842d2f4aa8a758b8bed5239802",
      "7d793b93989c4a9092c6928e9fc2eeed",
      "4746a86cb26e4c8dabb1ad3aab949466",
      "2684ac4b05344207ab3b6926c882b7c7",
      "f07d80d1360b4ee1b483c8b31f7ea28e",
      "7667ac159be24d3d937abea57e658dc9",
      "c17ba72d768d496da3475f72ede4d72f",
      "7fd6e9c55c10400aa45f4ca660d009a4",
      "73fdea5ec1434b81a41297cd992d1356",
      "29f8099cb2ea462ba22dc96872051017",
      "7c6fe79187dc42618fc3bc0e254f230c",
      "8fb3fd94c3b6424cadc7db6a6649b9a4",
      "185ff80d85ad4571ab2f5d6064e86146",
      "575b712039304bfbb6566bf69354bf98",
      "0b614e7d9e1f476d968dd6597f9645d7",
      "5a1fc8ac16e04a5895699a5f3826db12",
      "1c49075c83e14610b9693b41016dad63",
      "f71d385fc7f74d76bf25973b937f8ec1",
      "7e890a885a18492ab5ea805e5e567aa2",
      "50dbfe05a26d44d0b68b124129f15c24",
      "d29b53e77d144f50b21f0793fda2f4d3"
     ]
    },
    "id": "HlxZsnMHIF22",
    "outputId": "6fa0968b-949b-4def-f98d-5b46d5571b3c"
   },
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Comparing for Query 1\n",
    "query = \"What is the minimum income required for a personal loan?\"\n",
    "\n",
    "# Reference answer (from policy doc)\n",
    "reference = \"For salaried applicants, the minimum net monthly income is £1,200. For self-employed, it is £1,500 with stable income over 6 months.\"\n",
    "\n",
    "# Simulated LLM-only response (e.g., hallucinated or vague)\n",
    "llm_only = \"The minimum income requirement depends on the lender's criteria but is usually around £1,000.\"\n",
    "\n",
    "# Actual RAG response from Gemini\n",
    "rag_response = \"According to the Personal Loan Credit Policy provided:\\n\\n* **Salaried:** The minimum net monthly income is £1,500.\\n* **Self-employed:** The minimum net monthly income is £1,800, verified via a 6-month bank history.\"\n",
    "\n",
    "# Scoring both responses\n",
    "P = [rag_response]\n",
    "C = [llm_only]\n",
    "R = [reference]\n",
    "\n",
    "print(\"Evaluating RAG response:\")\n",
    "P_score, R_score, F1 = score(P, R, lang=\"en\", verbose=True)\n",
    "print(f\"BERTScore (RAG vs Reference): {F1[0].item():.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating LLM-only response:\")\n",
    "P_score, R_score, F1 = score(C, R, lang=\"en\", verbose=True)\n",
    "print(f\"BERTScore (LLM-only vs Reference): {F1[0].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "WsjMA3NwJSHS"
   },
   "outputs": [],
   "source": [
    "### BERTScore Evaluation Summary\n",
    "\n",
    "To quantify the semantic similarity between system responses and a reference answer, we applied **BERTScore** to compare both the RAG-generated output and a baseline LLM-only response.\n",
    "\n",
    "For the query:\n",
    "\n",
    "> *\"What is the minimum income required for a personal loan?\"*\n",
    "\n",
    "- **RAG vs Reference**: `BERTScore = 0.9008`\n",
    "- **LLM-only vs Reference**: `BERTScore = 0.8853`\n",
    "\n",
    "Although both models performed well, the RAG system produced a **more semantically accurate and complete answer**, as reflected in the slightly higher BERTScore. This supports the observation from rubric scoring that **RAG provides better grounding in domain-specific knowledge**, especially when the reference is known and traceable to policy documents.\n",
    "\n",
    "While the margin is modest in this case, the improvement is meaningful given that BERTScore is already a high-recall, high-overlap metric. Further benefits of RAG (e.g., hallucination resistance and traceability) are not fully captured by BERTScore but are reflected in qualitative evaluations.\n",
    "\n",
    "\n",
    "Summary:\n",
    "To supplement rubric-based evaluation, BERTScore was used to assess semantic similarity between system outputs and a human-written reference answer. For the query on minimum income for personal loans, the RAG system scored 0.9008, compared to 0.8853 for the LLM-only baseline. This confirms that the RAG response was more closely aligned with the intended answer, demonstrating improved semantic accuracy. Although the difference is modest, it reinforces the benefit of grounding responses in domain-specific documents and supports RAG's role in reducing ambiguity and improving answer faithfulness in regulated domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "m5n90JVp-pTi"
   },
   "outputs": [],
   "source": [
    "# 5. Future Scope\n",
    "\n",
    "While the current RAG system demonstrates strong performance in credit risk policy interpretation, there are several promising directions for future enhancement:\n",
    "\n",
    "First, the integration of reranking models (e.g., Cohere Rerank or BAAI bge-reranker) could further refine retrieval quality by reordering retrieved chunks based on query relevance. This would be especially useful when policy content overlaps across documents.\n",
    "\n",
    "Second, implementing multi-hop or recursive retrieval would allow the system to chain multiple pieces of evidence from different sources — enabling more complex reasoning, such as exception cases that depend on both eligibility and approval matrices.\n",
    "\n",
    "Third, the pipeline could be extended with a user-facing interface using Streamlit or Gradio, allowing non-technical users (e.g., analysts, loan officers) to interact with the assistant intuitively.\n",
    "\n",
    "Finally, as Gemini continues to evolve, its deeper integration with frameworks like LangChain could streamline the pipeline and open the door to more efficient, end-to-end enterprise deployment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPthSnqJTf1nKskw/3+nOMq",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
